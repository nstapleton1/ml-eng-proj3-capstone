{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Create a Customer Segmentation Report for Arvato Financial Services\n",
    "\n",
    "In this project, you will analyze demographics data for customers of a mail-order sales company in Germany, comparing it against demographics information for the general population. You'll use unsupervised learning techniques to perform customer segmentation, identifying the parts of the population that best describe the core customer base of the company. Then, you'll apply what you've learned on a third dataset with demographics information for targets of a marketing campaign for the company, and use a model to predict which individuals are most likely to convert into becoming customers for the company. The data that you will use has been provided by our partners at Bertelsmann Arvato Analytics, and represents a real-life data science task.\n",
    "\n",
    "If you completed the first term of this program, you will be familiar with the first part of this project, from the unsupervised learning project. The versions of those two datasets used in this project will include many more features and has not been pre-cleaned. You are also free to choose whatever approach you'd like to analyzing the data rather than follow pre-determined steps. In your work on this project, make sure that you carefully document your steps and decisions, since your main deliverable for this project will be a blog post reporting your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries here; add more as necessary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting icecream\n",
      "  Downloading https://files.pythonhosted.org/packages/8c/ec/821ef939e8e4f4306e7263afa7e2ce0b4c5da9e6e53d1cc97b01606035f8/icecream-2.0.0-py2.py3-none-any.whl\n",
      "Collecting asttokens>=2.0.1 (from icecream)\n",
      "  Downloading https://files.pythonhosted.org/packages/62/e9/247023d33dc110117b831cbfe47bb553e10d0edf92297ace745256402d42/asttokens-2.0.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pygments>=2.2.0 in /opt/conda/lib/python3.6/site-packages (from icecream) (2.2.0)\n",
      "Collecting executing>=0.3.1 (from icecream)\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/3d/2c2cf37d6194fa93c35e7ba6ab5aaa841a9b1b788fc322b01e53e0602049/executing-0.5.4-py3-none-any.whl\n",
      "Requirement already satisfied: colorama>=0.3.9 in /opt/conda/lib/python3.6/site-packages (from icecream) (0.3.9)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from asttokens>=2.0.1->icecream) (1.11.0)\n",
      "Installing collected packages: asttokens, executing, icecream\n",
      "Successfully installed asttokens-2.0.4 executing-0.5.4 icecream-2.0.0\n"
     ]
    }
   ],
   "source": [
    "#useful tool for debugging\n",
    "!pip install icecream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all other imports \n",
    "import icecream as ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful variables\n",
    "addtional_customer_cols = ['PRODUCT_GROUP', 'CUSTOMER_GROUP', 'ONLINE_PURCHASE']\n",
    "azdias_num_rows = 891221\n",
    "azdias_num_cols = 366\n",
    "customers_num_rows = 191652\n",
    "customers_num_cols = 369"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Get to Know the Data\n",
    "\n",
    "There are four data files associated with this project:\n",
    "\n",
    "- `Udacity_AZDIAS_052018.csv`: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns).\n",
    "- `Udacity_CUSTOMERS_052018.csv`: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns).\n",
    "- `Udacity_MAILOUT_052018_TRAIN.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns).\n",
    "- `Udacity_MAILOUT_052018_TEST.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns).\n",
    "\n",
    "Each row of the demographics files represents a single person, but also includes information outside of individuals, including information about their household, building, and neighborhood. Use the information from the first two files to figure out how customers (\"CUSTOMERS\") are similar to or differ from the general population at large (\"AZDIAS\"), then use your analysis to make predictions on the other two files (\"MAILOUT\"), predicting which recipients are most likely to become a customer for the mail-order company.\n",
    "\n",
    "The \"CUSTOMERS\" file contains three extra columns ('CUSTOMER_GROUP', 'ONLINE_PURCHASE', and 'PRODUCT_GROUP'), which provide broad information about the customers depicted in the file. The original \"MAILOUT\" file included one additional column, \"RESPONSE\", which indicated whether or not each recipient became a customer of the company. For the \"TRAIN\" subset, this column has been retained, but in the \"TEST\" subset it has been removed; it is against that withheld column that your final predictions will be assessed in the Kaggle competition.\n",
    "\n",
    "Otherwise, all of the remaining columns are the same between the three data files. For more information about the columns depicted in the files, you can refer to two Excel spreadsheets provided in the workspace. [One of them](./DIAS Information Levels - Attributes 2017.xlsx) is a top-level list of attributes and descriptions, organized by informational category. [The other](./DIAS Attributes - Values 2017.xlsx) is a detailed mapping of data values for each feature in alphabetical order.\n",
    "\n",
    "In the below cell, we've provided some initial code to load in the first two datasets. Note for all of the `.csv` data files in this project that they're semicolon (`;`) delimited, so an additional argument in the [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) call has been included to read in the data properly. Also, considering the size of the datasets, it may take some time for them to load completely.\n",
    "\n",
    "You'll notice when the data is loaded in that a warning message will immediately pop up. Before you really start digging into the modeling and analysis, you're going to need to perform some cleaning. Take some time to browse the structure of the data and look over the informational spreadsheets to understand the data values. Make some decisions on which features to keep, which features to drop, and if any revisions need to be made on data formats. It'll be a good idea to create a function with pre-processing steps, since you'll need to clean all of the datasets before you work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# load in the data\n",
    "azdias = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_AZDIAS_052018.csv', sep=';')\n",
    "customers = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_CUSTOMERS_052018.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#observe warning above - 5 columns have mixed types.  Then need to be identified and addressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azdias:  (891221, 366)\n",
      "customers:  (191652, 369)\n"
     ]
    }
   ],
   "source": [
    "#check the correct rows and cols are loaded\n",
    "print(\"azdias: \",azdias.shape)\n",
    "print(\"customers: \",customers.shape)\n",
    "\n",
    "assert azdias.shape[0] == 891221, \"azdias row count incorrect\"\n",
    "assert azdias.shape[1] == 366, \"azdias col count incorrect\"\n",
    "assert customers.shape[0] == 191652, \"azdias row count incorrect\"\n",
    "assert customers.shape[1] == 369, \"azdias col count incorrect\"\n",
    "\n",
    "#make a copy of the df\n",
    "azdias_org = azdias\n",
    "customers_org = customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the values and attributes\n",
    "attributes_value_ranges = pd.read_excel('DIAS Attributes - Values 2017.xlsx', header =1)\n",
    "attributes_descriptions = pd.read_excel('DIAS Information Levels - Attributes 2017.xlsx', header = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dias_values[\"Attribute\"] = dias_values[\"Attribute\"].ffill()\n",
    "dias_values.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printListDetails(l, desc, counter=1):\n",
    "    '''\n",
    "    '''\n",
    "    print (counter, desc, \":\", 'len:', len(l), ',', l, '\\n')\n",
    "    counter = counter + 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyse columns\n",
    "azdias_cols = azdias.columns\n",
    "customers_cols = customers.columns\n",
    "attributes_ranges_cols_list = attributes_value_ranges.Attribute\n",
    "attributes_descriptions_cols_list = attributes_descriptions.Attribute\n",
    "\n",
    "diff_customers_cols_versus_azdias_cols = list(set(customers_cols) - set(azdias_cols))\n",
    "diff_azdias_cols_versus_customers_cols = list(set(azdias_cols) - set(customers_cols))\n",
    "diff_attrib_ranges_cols_versus_customers_cols = list(set(attributes_ranges_cols_list) - set(customers_cols))\n",
    "diff_customers_cols_versus_attrib_ranges_cols = list(set(customers_cols) - set(attributes_ranges_cols_list))\n",
    "diff_attrib_ranges_cols_versus_azdias_cols = list(set(attributes_ranges_cols_list) - set(azdias_cols))\n",
    "diff_azdias_cols_versus_attrib_ranges_cols = list(set(azdias_cols) - set(attributes_ranges_cols_list))\n",
    "diff_between_missing_col_customers_azdias = list(set(diff_attrib_ranges_cols_versus_customers_cols) - set(diff_attrib_ranges_cols_versus_azdias_cols))\n",
    "diff_between_missing_col_azdias_customers = list(set(diff_attrib_ranges_cols_versus_azdias_cols) - set(diff_attrib_ranges_cols_versus_customers_cols))\n",
    "  \n",
    "counter=1\n",
    "counter=printListDetails(diff_customers_cols_versus_azdias_cols, \"Customer versus azdias lists (expect 3 cols)\", counter)\n",
    "counter=printListDetails(diff_azdias_cols_versus_customers_cols, \"Azdias versus customers lists (expect 0 cols)\", counter)\n",
    "counter=printListDetails(diff_attrib_ranges_cols_versus_customers_cols, \"Attributes versus customers lists\", counter)\n",
    "counter=printListDetails(diff_customers_cols_versus_attrib_ranges_cols, \"Customers versus Attributes lists\", counter)\n",
    "counter=printListDetails(diff_attrib_ranges_cols_versus_azdias_cols, \"Attributes versus azdias lists\", counter)\n",
    "counter=printListDetails(diff_azdias_cols_versus_attrib_ranges_cols, \"Azdias versus Attributes lists\", counter)\n",
    "counter=printListDetails(diff_between_missing_col_customers_azdias, \"Excess missing cols customers versus azdias (expect 0 cols)\", counter)\n",
    "counter=printListDetails(diff_between_missing_col_azdias_customers, \"Excess missing cols azdias versus customers (expect 0 cols):\", counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. This is expected: we were told that the customer df has three extra columns\n",
    "    \n",
    "2. Expected: the azdias df does not have more cols than the customers df\n",
    "\n",
    "3. Surprising: there are 43 more attribs listed than are used on the customers df\n",
    "\n",
    "4. Very surprising: there are 97 cols on the customers df that are not described in the attribs listed\n",
    "\n",
    "5. Surprising: there are 43 more attribs listed than are used on the azdias df\n",
    "\n",
    "6. Very surprising: there are 94 cols on the azdias df that are not described in the attribs listed\n",
    "\n",
    "7. Expected: the 'missing' cols are consistent \n",
    "\n",
    "8. Expected: the 'missing' cols are consistent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    1\n",
      "2    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#jotter\n",
    "\n",
    "df = pd.DataFrame([[1, 2, np.nan], [np.nan, 3, 4],[1, 2, 3]])\n",
    "#%timeit -n 1000 df.isnull().sum(axis=1)\n",
    "x = df.isnull().sum(axis=1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the attributes that have unknown as a possible value\n",
    "attributes_with_unknowns = attributes_value_ranges.loc[attributes_value_ranges['Meaning'] == 'unknown']\n",
    "\n",
    "\n",
    "#attributes_with_unknowns.Value\n",
    "#attributes_with_unknowns.Value.describe()\n",
    "#titanic[titanic[\"Pclass\"].isin([2, 3])]\n",
    "#df = pd.DataFrame([-1,0,1,2,3])\n",
    "#df.columns = ['val']\n",
    "#df.iloc[0]['val']\n",
    "#df\n",
    "#df[df[\"val\"].isin([2, 3])]\n",
    "#x=df[df[\"val\"].isin([2, 3])]\n",
    "#print(x)\n",
    "\n",
    "#attributes_with_unknowns.iloc[1][2]\n",
    "#attributes_with_unknowns\n",
    "\n",
    "#if df.iloc[0]['val'].isin(attributes_with_unknowns.iloc[1][2]):\n",
    "#mList = [int(e) if e.isdigit() else e for e in mStr.split(',')]\n",
    "#x = df.iloc[0].isin([int(e) if e.isdigit() else e for e in attributes_with_unknowns.iloc[1][2].split(',')])\n",
    "#print(x)\n",
    "#attributes_with_unknowns = attributes_value_ranges.loc[dias_values['Meaning'] == 'unknown' | dias_values['Meaning'] == 'no transactions known' | dias_values['Meaning'] == 'no transactions known']\n",
    "attributes_with_unknowns = attributes_value_ranges.loc[attributes_value_ranges['Meaning'] == 'unknown']\n",
    "attributes_with_unknowns = attributes_with_unknowns.append(attributes_value_ranges.loc[attributes_value_ranges['Meaning'] == 'no transactions known'])\n",
    "attributes_with_unknowns = attributes_with_unknowns.append(attributes_value_ranges.loc[attributes_value_ranges['Meaning'] == 'no transaction known'])\n",
    "attributes_with_unknowns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(287, 4)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = df.drop(df[df.score < 50].index)\n",
    "#df = attributes_with_unknowns\n",
    "#df = df.drop(df[df.Attribute == 'BIP_FLAG'])\n",
    "#df_filtered = df[df['Age'] >= 25] \n",
    "attributes_with_unknowns = attributes_with_unknowns[attributes_with_unknowns['Attribute'] != 'BIP_FLAG'] \n",
    "#attributes_with_unknowns = attributes_with_unknowns.drop(attributes_with_unknowns['Attribute'] == 'BIP_FLAG')\n",
    "#df_filtered.shape\n",
    "attributes_with_unknowns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unknown', 'unknown / no main age detectable', 'no transactions known', 'no transaction known', 'residental building buildings without actually known household', 'mixed building without actually known household or comapny ', 'company building w/o known company ', 'mixed building without actually known household ', 'mixed building without actually known company ']\n"
     ]
    }
   ],
   "source": [
    "#df[df['model'].str.contains('ac')]\n",
    "#x = attributes_value_ranges.loc[dias_values['Meaning'].str.contains('known')]\n",
    "#x =attributes_value_ranges.loc[dias_values['Meaning'] == 'unknown']\n",
    "x = attributes_value_ranges.Meaning.unique()\n",
    "\n",
    "#for i in attributes_value_ranges.Meaning.unique():\n",
    "    #print(i)\n",
    "#strings_with_substring = [string for string in x if 'known' in string]\n",
    "\n",
    "unknown_terms = []\n",
    "for i in attributes_value_ranges.Meaning.unique():\n",
    "    if ('KNOWN' in str(i).upper()):\n",
    "        unknown_terms.append(i)\n",
    "\n",
    "print(unknown_terms)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1]\n"
     ]
    }
   ],
   "source": [
    "def value_range_for_attribute(df, sKey_Name):\n",
    "    '''\n",
    "    '''\n",
    "    value_list = []\n",
    "    cell_value = str(df.loc[df[\"Attribute\"] == sKey_Name, \"Value\"].values[0]).split(',')\n",
    "    for i in cell_value:\n",
    "        value_list.append(int(i))\n",
    "    \n",
    "    return value_list\n",
    "\n",
    "print(value_range_for_attribute(attributes_with_unknowns, 'WOHNLAGE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#azdias_sub.BIP_FLAG.unique()\n",
    "list(azdias_sub.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "azdias_sub = azdias.head(15)\n",
    "attribute_list = attributes_with_unknowns['Attribute'].tolist()\n",
    "#azdias_sub\n",
    "\n",
    "def replace_unknown_values_with_nan(df, list_of_attribs):\n",
    "    '''\n",
    "    '''\n",
    "    attribs_not_present = []\n",
    "    for attribute in list_of_attribs:\n",
    "        print(\"replace_unknown_values_with_nan:\", attribute)\n",
    "        #df['CustomRating'] = df.apply(lambda x: custom_rating(x['Genre'],x['Rating']),axis=1)\n",
    "        #nan_counts['percent_nan_missing'] = nan_counts[0].apply(lambda x: x*100/len(df))\n",
    "        #df['col1'] = df.apply(lambda x: complex_function(x['col1']), axis=1)\n",
    "        #df[attribute] = df[attribute].apply(lambda x:update_cell_to_nan_for_unknown(attribute, x))\n",
    "        #df['col1'] = df.apply(lambda x: complex_function(x['col1']), axis=1)\n",
    "        #df[attrib] = df[attrib].apply(replace_unkown_with_nan, args=(unkown))\n",
    "        #df[attribute] = df[attribute].apply(update_cell_to_nan_for_unknown, args=(attribute))\n",
    "        if attribute in df.columns:\n",
    "            df[attribute] = df.apply(lambda x:update_cell_to_nan_for_unknown(attribute, x[attribute]),axis=1)\n",
    "        else:\n",
    "            attribs_not_present.append(attribute)\n",
    "\n",
    "def update_cell_to_nan_for_unknown(colname, curr_value):\n",
    "    print(\"update_cell_to_nan_for_unknown: \", colname)\n",
    "    print(\"update_cell_to_nan_for_unknown: \", curr_value)\n",
    "    #if curr_value.isin(value_range_for_attribute(attributes_with_unknowns,colname)):\n",
    "    if curr_value in (value_range_for_attribute(attributes_with_unknowns,colname)):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return curr_value\n",
    "\n",
    "#this is working\n",
    "#print(update_cell_to_nan_for_unknown(\"AGER_TYP\",-1))\n",
    "\n",
    "#azdias[\"AGER_TYP\"] = azdias[\"AGER_TYP\"].apply(replace_unkown_with_nan, args=(ager_typ_unkwon))\n",
    "#azdias_sub[\"AGER_TYP\"] = azdias[\"AGER_TYP\"].apply(update_cell_to_nan_for_unknown, args=(value_range_for_attribute(\"AGER_TYPE\")))\n",
    "replace_unknown_values_with_nan(azdias_sub, attribute_list)\n",
    "#azdias_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2    3  percent_nan_missing  percent_nan_missingxx\n",
      "0  1.0  2.0  NaN  4.0                    1                   25.0\n",
      "1  NaN  NaN  3.0  4.0                    2                   50.0\n",
      "2  1.0  2.0  3.0  NaN                    1                   25.0\n"
     ]
    }
   ],
   "source": [
    "#jotter\n",
    "\n",
    "df = pd.DataFrame([[1, 2, np.nan, 4], [np.nan, np.nan, 3, 4],[1, 2, 3]])\n",
    "#%timeit -n 1000 df.isnull().sum(axis=1)\n",
    "#x = pd.DataFrame(df.isnull().sum(axis=1))\n",
    "#y = list(df.isnull().sum(axis=1))\n",
    "#x = df.isnull().sum(axis=1)\n",
    "#print(y)\n",
    "#print(x)\n",
    "#print(list(x.iloc[:,-1]))\n",
    "print(calculate_percentage_nan_per_row(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "azdias_sub = azdias.head(15)\n",
    "\n",
    "def calculate_percentage_nan_per_row(df):\n",
    "    '''\n",
    "    '''\n",
    "    rowcount = df.shape[1] \n",
    "    nan_count = list(df.isnull().sum(axis=1))\n",
    "    df['percent_nan_missing'] = nan_count\n",
    "    df['percent_nan_missingxx'] = df['percent_nan_missing'].apply(lambda x: x*100/rowcount)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Information level</th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Description</th>\n",
       "      <th>Additional notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>AGER_TYP</td>\n",
       "      <td>best-ager typology</td>\n",
       "      <td>in cooperation with Kantar TNS; the informatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>Person</td>\n",
       "      <td>ALTERSKATEGORIE_GROB</td>\n",
       "      <td>age through prename analysis</td>\n",
       "      <td>modelled on millions of first name-age-referen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ANREDE_KZ</td>\n",
       "      <td>gender</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>CJT_GESAMTTYP</td>\n",
       "      <td>Customer-Journey-Typology relating to the pref...</td>\n",
       "      <td>relating to the preferred information, marketi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>FINANZ_MINIMALIST</td>\n",
       "      <td>financial typology: low financial interest</td>\n",
       "      <td>Gfk-Typology based on a representative househo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>FINANZ_SPARER</td>\n",
       "      <td>financial typology: money saver</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>FINANZ_VORSORGER</td>\n",
       "      <td>financial typology: be prepared</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>FINANZ_ANLEGER</td>\n",
       "      <td>financial typology: investor</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>FINANZ_UNAUFFAELLIGER</td>\n",
       "      <td>financial typology: unremarkable</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>FINANZ_HAUSBAUER</td>\n",
       "      <td>financial typology: main focus is the own house</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Information level              Attribute  \\\n",
       "NaN               NaN               AGER_TYP   \n",
       "NaN            Person   ALTERSKATEGORIE_GROB   \n",
       "NaN               NaN              ANREDE_KZ   \n",
       "NaN               NaN          CJT_GESAMTTYP   \n",
       "NaN               NaN      FINANZ_MINIMALIST   \n",
       "NaN               NaN          FINANZ_SPARER   \n",
       "NaN               NaN       FINANZ_VORSORGER   \n",
       "NaN               NaN         FINANZ_ANLEGER   \n",
       "NaN               NaN  FINANZ_UNAUFFAELLIGER   \n",
       "NaN               NaN       FINANZ_HAUSBAUER   \n",
       "\n",
       "                                           Description  \\\n",
       "NaN                                 best-ager typology   \n",
       "NaN                      age through prename analysis    \n",
       "NaN                                             gender   \n",
       "NaN  Customer-Journey-Typology relating to the pref...   \n",
       "NaN         financial typology: low financial interest   \n",
       "NaN                    financial typology: money saver   \n",
       "NaN                    financial typology: be prepared   \n",
       "NaN                       financial typology: investor   \n",
       "NaN                   financial typology: unremarkable   \n",
       "NaN    financial typology: main focus is the own house   \n",
       "\n",
       "                                      Additional notes  \n",
       "NaN  in cooperation with Kantar TNS; the informatio...  \n",
       "NaN  modelled on millions of first name-age-referen...  \n",
       "NaN                                                NaN  \n",
       "NaN  relating to the preferred information, marketi...  \n",
       "NaN  Gfk-Typology based on a representative househo...  \n",
       "NaN                                                NaN  \n",
       "NaN                                                NaN  \n",
       "NaN                                                NaN  \n",
       "NaN                                                NaN  \n",
       "NaN                                                NaN  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dias_attribs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the cols with mixed data types\n",
    "def findColumnsWithMixedDataTypes(df):\n",
    "    '''\n",
    "    '''\n",
    "    columnsWithMixedDataTypes = []\n",
    "    for col in df.columns:\n",
    "        weird = (df[[col]].applymap(type) != df[[col]].iloc[0].apply(type)).any(axis=1)\n",
    "        if len(df[weird]) > 0:\n",
    "            #print(col)\n",
    "            columnsWithMixedDataTypes.append(col)\n",
    "    \n",
    "    return columnsWithMixedDataTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CAMEO_DEUG_2015', 'CAMEO_INTL_2015'], dtype='object')\n",
      "Index(['CAMEO_DEUG_2015', 'CAMEO_INTL_2015'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Error message on loading: DtypeWarning: Columns (18,19) have mixed types\n",
    "#=> these are the only two we need to worry about\n",
    "#first check do they appear in the lists above\n",
    "print(azdias.columns[18:20])\n",
    "print(customers.columns[18:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_dtype_colnames = ['CAMEO_DEUG_2015', 'CAMEO_INTL_2015']\n",
    "#note: CAMEO_DEUG_2015 is xxxx on the Attributes s/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at values in these warning cols\n",
    "print(pd.value_counts(azdias.CAMEO_DEUG_2015))\n",
    "print(pd.value_counts(azdias.CAMEO_INTL_2015))\n",
    "\n",
    "print(pd.value_counts(customers.CAMEO_DEUG_2015))\n",
    "print(pd.value_counts(customers.CAMEO_INTL_2015))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to update these two columns\n",
    "#NB CAMEO_INTL_2015 is CAMEO_DEINTL_2015 in DIAS Attribute Values 2017 s/s \n",
    "#CAMEO_INTL_2015: object type can be set to -1\n",
    "#CAMEO_DEUG_2015: object type can be set to -1\n",
    "#also will cast ALL values to ints\n",
    "\n",
    "def updateMixedDataTypeColumns(df, list_of_cols):\n",
    "    '''\n",
    "    '''  \n",
    "    for col in list_of_cols:\n",
    "        #first replace the object types with -1\n",
    "        #df[col].replace({\"X\": -1, \"XX\": -1}, inplace=True)\n",
    "        #first replace the object types with np.nan\n",
    "        df[col].replace({\"X\": np.nan, \"XX\": np.nan}, inplace=True)\n",
    "        #set Nan to -1\n",
    "        df[col] = df[col].fillna(-1)\n",
    "        #cast everything to int\n",
    "        df[col] = df[col].astype(int)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "updateMixedDataTypeColumns(azdias, mixed_dtype_colnames)\n",
    "updateMixedDataTypeColumns(customers, mixed_dtype_colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_percent_nan_missing(df):\n",
    "    '''\n",
    "    '''\n",
    "    \n",
    "    #get a count of the nans\n",
    "    nan_counts = pd.DataFrame(df.isna().sum())\n",
    "    #calculate percent_nan_missing\n",
    "    nan_counts['percent_nan_missing'] = nan_counts[0].apply(lambda x: x*100/len(df))\n",
    "    #reset the indexes\n",
    "    nan_counts.reset_index(level=0, inplace=True)\n",
    "    #set column names\n",
    "    nan_counts.columns = ['feature', 'count', 'percent_nan_missing']\n",
    "    #drop sum column\n",
    "    nan_counts=nan_counts.drop(columns=['count'])\n",
    "    \n",
    "    return nan_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_nan_cols = calc_percent_nan_missing(azdias)\n",
    "customers_nan_cols = calc_percent_nan_missing(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_compare_nan_percentages(df1, df1_name, df2, df2_name):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(10,80))\n",
    "\n",
    "    fig.suptitle(\"NaN %s\")\n",
    "\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax.set_title(df1_name)\n",
    "    sns.barplot(y=\"feature\", x=\"percent_nan_missing\", data=df1[df1.percent_nan_missing>=0], ax= ax)\n",
    "\n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.set_title(df2_name)\n",
    "    sns.barplot(y=\"feature\", x=\"percent_nan_missing\", data=df2[df2.percent_nan_missing>=0], ax= ax)\n",
    "\n",
    "    #fig.tight_layout(rect=[0, 0.03, 1, 0.975])\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_compare_nan_percentages(azdias_nan_cols, 'azdias', customers_nan_cols, 'customers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_distribution_of_num_attributes(df):\n",
    "    '''\n",
    "    '''\n",
    "    num_attributes = df.select_dtypes(exclude='object')\n",
    "\n",
    "    fig = plt.figure(figsize=(12,18))\n",
    "    for i in range(len(num_attributes.columns)):\n",
    "        fig.add_subplot(9,4,i+1)\n",
    "        sns.distplot(num_attributes.iloc[:,i].dropna())\n",
    "        plt.xlabel(num_attributes.columns[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_distribution_of_num_attributes(azdias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_list_of_nans_below_threshold(df,threshold):\n",
    "    l = []\n",
    "    l = list(df.drop(df.loc[:,list((100*(df.isnull().sum()/len(df.index))>=threshold))].columns, 1).columns.values)\n",
    "    print(\"# Columns having more than %s percent missing values:\"%threshold,(df.shape[1] - len(l)))\n",
    "    print(\"Columns:\\n\",list(set(list((df.columns.values))) - set(l)))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the numeric attributes #1\n",
    "num_attributes = customers.select_dtypes(exclude='object')\n",
    "num_attributes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the numeric attributes #2\n",
    "num_attributes.isna().sum().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at non-numeric attributes #1\n",
    "non_num_attributes = customers.select_dtypes(include='object')\n",
    "non_num_attributes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at non-numeric attributes #2\n",
    "non_num_attributes.head\n",
    "#non_num_attributes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CAMEO_INTL_2015    50428\n",
       "CAMEO_DEUG_2015    50428\n",
       "CAMEO_DEU_2015     50428\n",
       "OST_WEST_KZ        49927\n",
       "EINGEFUEGT_AM      49927\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_num_attributes.isna().sum().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Customer Segmentation Report\n",
    "\n",
    "The main bulk of your analysis will come in this part of the project. Here, you should use unsupervised learning techniques to describe the relationship between the demographics of the company's existing customers and the general population of Germany. By the end of this part, you should be able to describe parts of the general population that are more likely to be part of the mail-order company's main customer base, and which parts of the general population are less so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Supervised Learning Model\n",
    "\n",
    "Now that you've found which parts of the population are more likely to be customers of the mail-order company, it's time to build a prediction model. Each of the rows in the \"MAILOUT\" data files represents an individual that was targeted for a mailout campaign. Ideally, we should be able to use the demographic information from each individual to decide whether or not it will be worth it to include that person in the campaign.\n",
    "\n",
    "The \"MAILOUT\" data has been split into two approximately equal parts, each with almost 43 000 data rows. In this part, you can verify your model with the \"TRAIN\" partition, which includes a column, \"RESPONSE\", that states whether or not a person became a customer of the company following the campaign. In the next part, you'll need to create predictions on the \"TEST\" partition, where the \"RESPONSE\" column has been withheld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_train = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TRAIN.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Kaggle Competition\n",
    "\n",
    "Now that you've created a model to predict which individuals are most likely to respond to a mailout campaign, it's time to test that model in competition through Kaggle. If you click on the link [here](http://www.kaggle.com/t/21e6d45d4c574c7fa2d868f0e8c83140), you'll be taken to the competition page where, if you have a Kaggle account, you can enter. If you're one of the top performers, you may have the chance to be contacted by a hiring manager from Arvato or Bertelsmann for an interview!\n",
    "\n",
    "Your entry to the competition should be a CSV file with two columns. The first column should be a copy of \"LNR\", which acts as an ID number for each individual in the \"TEST\" partition. The second column, \"RESPONSE\", should be some measure of how likely each individual became a customer â€“ this might not be a straightforward probability. As you should have found in Part 2, there is a large output class imbalance, where most individuals did not respond to the mailout. Thus, predicting individual classes and using accuracy does not seem to be an appropriate performance evaluation method. Instead, the competition will be using AUC to evaluate performance. The exact values of the \"RESPONSE\" column do not matter as much: only that the higher values try to capture as many of the actual customers as possible, early in the ROC curve sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_test = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TEST.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
